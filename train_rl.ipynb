{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-23T22:45:50.227105Z",
     "start_time": "2025-02-23T22:45:47.854969Z"
    }
   },
   "source": [
    "# %cd /home/gsantoss/frodo\n",
    "\n",
    "from board3 import Board3, sqr_distance, empty_cells, tod_cells\n",
    "from controller3 import ActionController, MW_CELLS\n",
    "import time\n",
    "from heapq import heappush, heappop\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.categorical import Categorical\n",
    "from collections import Counter, deque\n",
    "from mcts import search\n",
    "from nnl import gather_history\n",
    "import concurrent.futures\n",
    "import torch.multiprocessing as tmp"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:45:53.707744Z",
     "start_time": "2025-02-23T22:45:53.702445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_emb(board: Board3):\n",
    "\n",
    "    py, px = board.get_player_position()\n",
    "    ey, ex = board.get_enemy_position()\n",
    "    ty, tx = board.get_todd_position()\n",
    "\n",
    "    mws = [0] * 16\n",
    "\n",
    "    for (y, x) in board.mw:\n",
    "        mws[y * 4 + x] = 1\n",
    "\n",
    "    pe = torch.Tensor([[py, px, ey, ex, ty, tx]]) / 3\n",
    "    mwe = torch.Tensor([mws])\n",
    "\n",
    "    return torch.cat([pe, mwe], dim=1)\n",
    "\n",
    "\n",
    "def emb_mem(mem, nc=2):\n",
    "    fe = []\n",
    "    for b, a in mem:\n",
    "        e1 = to_emb(b)\n",
    "        e2 = nn.functional.one_hot(torch.LongTensor([a]), num_classes=nc)\n",
    "        fe.append(torch.cat([e1, e2], dim=1))\n",
    "\n",
    "    return torch.cat(fe, dim=0).unsqueeze(0)"
   ],
   "id": "689d3375719d645b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:46:00.791826Z",
     "start_time": "2025-02-23T22:46:00.782769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pos_encode(max_len, d_model, dtype=torch.float32):\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, d_model, dtype=dtype)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "\n",
    "class FP(nn.Module):\n",
    "    def __init__(self, n_dim, a_space, m_space, e_dim=384, ff_dim=1024, n_layers=4, n_heads=12, max_len=15):\n",
    "        super(FP, self).__init__()\n",
    "\n",
    "        self.fl = nn.Sequential(\n",
    "            nn.Linear(n_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, e_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.fh = nn.Sequential(\n",
    "            nn.Linear(m_space, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, e_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "        )\n",
    "\n",
    "        self.dec_l = nn.TransformerDecoderLayer(d_model=e_dim, nhead=n_heads, dim_feedforward=ff_dim, batch_first=True)\n",
    "        self.dec = nn.TransformerDecoder(self.dec_l, num_layers=n_layers)\n",
    "\n",
    "        self.pe = pos_encode(max_len, e_dim)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(e_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, a_space),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.fv = nn.Sequential(\n",
    "            nn.Linear(e_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        nx = self.fl(x)\n",
    "        nh = self.fh(h) + self.pe[:h.shape[1], :].unsqueeze(0).to(h.device)\n",
    "        hidden = self.dec(nx.unsqueeze(1), nh).mean(dim=1)\n",
    "        return self.fc(hidden), self.fv(hidden)\n"
   ],
   "id": "a4afaa8ee610c065",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:46:02.237954Z",
     "start_time": "2025-02-23T22:46:02.228643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class NNT:\n",
    "\n",
    "    def __init__(self, board, player, m1, m2):\n",
    "        self.board = board\n",
    "        self.controller = ActionController(board)\n",
    "        self.children = None\n",
    "        self.player = player\n",
    "        self.action = None\n",
    "        self.prob = 0\n",
    "        self.value = 0\n",
    "        self.n = 0\n",
    "        self.m1 = m1\n",
    "        self.m2 = m2\n",
    "        pass\n",
    "\n",
    "\n",
    "    def get_winner(self):\n",
    "        if self.controller.is_win():\n",
    "            return 1\n",
    "        if self.controller.is_lose():\n",
    "            return -1\n",
    "        if self.controller.is_block():\n",
    "            return -1\n",
    "\n",
    "        return None\n",
    "\n",
    "    def search(self, fp):\n",
    "\n",
    "        winner = self.get_winner()\n",
    "        if winner is not None:\n",
    "            self.n += 1\n",
    "            self.value = -winner\n",
    "            return self.value\n",
    "\n",
    "\n",
    "        if self.children is None:\n",
    "            self.expand(fp)\n",
    "\n",
    "            return -self.value\n",
    "\n",
    "        cs = sum([x.n for x in self.children])\n",
    "        sv = max(self.children, key=lambda x: x.value + 0.5 * x.prob * (math.sqrt(cs ) / (x.n + 1)))\n",
    "        res = sv.search(fp)\n",
    "        self.value = (self.value * self.n + res) / (self.n + 1)\n",
    "        self.n += 1\n",
    "\n",
    "        return -res\n",
    "\n",
    "    def expand(self, fp):\n",
    "        self.children = []\n",
    "\n",
    "        if self.player == 1:\n",
    "            m = self.m1 + 0\n",
    "        else:\n",
    "            m = self.m2 + 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o, v = fp(to_emb(self.board), m)\n",
    "\n",
    "        vs = []\n",
    "        for p in self.controller.get_available_moves():\n",
    "            b = self.board.copy()\n",
    "            ActionController(b).execute_action(p)\n",
    "            b.step(500)\n",
    "\n",
    "            if self.player == 1:\n",
    "                m1 = torch.cat([self.m1[:, 1:, :], emb_mem([(b, p)], ActionController.get_action_space())], dim=1)\n",
    "                m2 = self.m2\n",
    "            else:\n",
    "                m1 = self.m1\n",
    "                m2 = torch.cat([self.m2[:, 1:, :], emb_mem([(b, p)], ActionController.get_action_space())], dim=1)\n",
    "\n",
    "            nt = NNT(b, -self.player, m1, m2)\n",
    "            nt.action = p\n",
    "            nt.prob = o[:, p].exp().item()\n",
    "            nt.value = 0\n",
    "\n",
    "            if nt.controller.is_win():\n",
    "                nt.value = 1\n",
    "            elif nt.controller.is_lose():\n",
    "                nt.value = -1\n",
    "            elif nt.controller.is_block():\n",
    "                nt.value = -1\n",
    "\n",
    "            nt.n = 1\n",
    "            vs.append(nt.prob)\n",
    "            b.swap_enemy()\n",
    "            self.children.append(nt)\n",
    "\n",
    "        self.value = v.item()\n",
    "        self.n = len(vs)\n",
    "\n",
    "    def get_policy(self):\n",
    "        p = [0] * ActionController.get_action_space()\n",
    "        for x in self.children:\n",
    "            p[x.action] = x.n\n",
    "\n",
    "        return torch.Tensor(p) / sum(p)\n"
   ],
   "id": "3407143a85b13a76",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:46:03.531844Z",
     "start_time": "2025-02-23T22:46:03.512973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def run_episode(fp, mem_length = 10, frames=100, mcts_iter=50):\n",
    "    fp.eval()\n",
    "    b = Board3(walk_time=200)\n",
    "\n",
    "    y1 = emb_mem([(b, 0)] * mem_length, ActionController.get_action_space())\n",
    "    b.swap_enemy()\n",
    "    y2 = emb_mem([(b, 0)] * mem_length, ActionController.get_action_space())\n",
    "    b.swap_enemy()\n",
    "\n",
    "    nt = NNT(b, 1, y1, y2)\n",
    "\n",
    "    h = []\n",
    "\n",
    "    for _ in range(frames):\n",
    "        for _ in range(mcts_iter):\n",
    "            nt.search(fp)\n",
    "\n",
    "        mt = max(nt.children, key=lambda x: x.n)\n",
    "        nb = mt.board.copy()\n",
    "        nb.swap_enemy()\n",
    "\n",
    "        if nt.player == 1:\n",
    "            h.append((to_emb(nb), nt.m1, nt.get_policy()))\n",
    "        else:\n",
    "            h.append((to_emb(nb), nt.m2, nt.get_policy()))\n",
    "\n",
    "        act = ActionController(nb)\n",
    "\n",
    "        if act.is_win():\n",
    "            r = reversed([1 if i % 2 == 0 else -1 for i in range(len(h))])\n",
    "            return [(x, y, z, v) for (x, y, z), v in zip(h, r)]\n",
    "        elif act.is_lose():\n",
    "            r = reversed([-1 if i % 2 == 0 else 1 for i in range(len(h))])\n",
    "            return [(x, y, z, v) for (x, y, z), v in zip(h, r)]\n",
    "        elif act.is_block():\n",
    "            r = reversed([-1 if i % 2 == 0 else 0 for i in range(len(h))])\n",
    "            return [(x, y, z, v) for (x, y, z), v in zip(h, r)]\n",
    "\n",
    "        nt = mt\n",
    "\n",
    "    return [(x, y, z, 0) for x, y, z in h]\n",
    "\n",
    "\n",
    "def train_nn(data, fp, epochs = 5, lr=0.001, batch_size=64):\n",
    "    b, m, p, r = list(zip(*data))\n",
    "    dataset = TensorDataset(torch.cat(b, dim=0), torch.cat(m, dim=0), torch.stack(p), torch.Tensor(r))\n",
    "    nf = nfp()\n",
    "    # nf.load_state_dict(fp.state_dict(), strict=False)\n",
    "    # nf.load_state_dict(torch.load('models/fp.pth', map_location=torch.device('cpu'), weights_only=True), strict=False)\n",
    "\n",
    "    optimizer = optim.AdamW(nf.parameters(), lr=lr)\n",
    "    crit1 = nn.CrossEntropyLoss()\n",
    "    crit2 = nn.MSELoss()\n",
    "\n",
    "    lh = []\n",
    "    for _ in range(epochs):\n",
    "        el = []\n",
    "        for b, m, p, r in DataLoader(dataset, batch_size=batch_size, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            o, v = nf(b, m)\n",
    "            l1 = crit1(o, p)\n",
    "            l2 = crit2(v.squeeze(1), r)\n",
    "            l = l1 + l2\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            el.append(l.item())\n",
    "        lh.append(sum(el) / len(el))\n",
    "\n",
    "\n",
    "    # plt.plot(lh)\n",
    "    # plt.show()\n",
    "    return nf\n",
    "\n",
    "\n",
    "def run_battle(fp, nf, mem_length = 10, frames=100):\n",
    "\n",
    "    fp.eval()\n",
    "    nf.eval()\n",
    "\n",
    "    b = Board3(walk_time=200)\n",
    "\n",
    "    y1 = emb_mem([(b, 0)] * mem_length, ActionController.get_action_space())\n",
    "    b.swap_enemy()\n",
    "    y2 = emb_mem([(b, 0)] * mem_length, ActionController.get_action_space())\n",
    "    b.swap_enemy()\n",
    "\n",
    "    act = ActionController(b)\n",
    "\n",
    "    for _ in range(frames):\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o, v = fp(to_emb(b), y1)\n",
    "            a1 = o.exp().argmax().item()\n",
    "            y1 = torch.cat([y1[:, 1:, :], emb_mem([(b, a1)], ActionController.get_action_space())], dim=1)\n",
    "        act.execute_action(a1)\n",
    "        b.step(500)\n",
    "\n",
    "        if act.is_win():\n",
    "            return 1\n",
    "        elif act.is_lose():\n",
    "            return -1\n",
    "        elif act.is_block():\n",
    "            return -1\n",
    "\n",
    "        b.swap_enemy()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o, v = nf(to_emb(b), y2)\n",
    "            a2 = o.exp().argmax().item()\n",
    "            y2 = torch.cat([y2[:, 1:, :], emb_mem([(b, a2)], ActionController.get_action_space())], dim=1)\n",
    "\n",
    "        act.execute_action(a2)\n",
    "        b.step(500)\n",
    "\n",
    "        if act.is_win():\n",
    "            return -1\n",
    "        elif act.is_lose():\n",
    "            return 1\n",
    "        elif act.is_block():\n",
    "            return 1\n",
    "\n",
    "        b.swap_enemy()\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def eval_nn(fp, nf, n = 40):\n",
    "    w = 0\n",
    "    d = 0\n",
    "    l = 0\n",
    "    for _ in range(n):\n",
    "        r = run_battle(fp, nf)\n",
    "        if r == 1:\n",
    "            w += 1\n",
    "        elif r == 0:\n",
    "            d += 1\n",
    "        else:\n",
    "            l += 1\n",
    "\n",
    "    return w / n, d / n, l / n\n",
    "\n",
    "\n",
    "def nfp():\n",
    "    b = Board3(walk_time=200)\n",
    "    x = to_emb(b)\n",
    "    y = emb_mem([(b, 0)] * 2, ActionController.get_action_space())\n",
    "    return FP(x.shape[1], ActionController.get_action_space(), y.shape[-1])"
   ],
   "id": "61911aa5e21092de",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:46:05.472411Z",
     "start_time": "2025-02-23T22:46:05.469654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fp = nfp()\n",
    "#\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     futures = set()\n",
    "#     for _ in range(1):\n",
    "#         futures.add(executor.submit(run_episode, fp))\n",
    "#\n",
    "#     done, futures = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "#     for fut in done:\n",
    "#         result = fut.result()\n",
    "#         print(result)\n",
    "# run_episode(fp)\n",
    "\n",
    "# p = tqdm(total=1000)\n",
    "#\n",
    "# data = []\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     futures = set()\n",
    "#     for _ in range(12):\n",
    "#         futures.add(executor.submit(run_episode, fp))\n",
    "#\n",
    "#     while len(data) < 1000:\n",
    "#         # Wait until at least one future is completed.\n",
    "#         done, futures = concurrent.futures.wait(futures, return_when=concurrent.futures.FIRST_COMPLETED)\n",
    "#         for fut in done:\n",
    "#             result = fut.result()\n",
    "#             if result:\n",
    "#                 data.extend(result)\n",
    "#                 p.update(len(result))\n",
    "#             # Submit a new simulation to keep the pool busy.\n",
    "#             futures.add(executor.submit(run_episode, fp))\n",
    "#             # Break early if we have reached the desired amount.\n",
    "#             if len(data) >= 1000:\n",
    "#                 break"
   ],
   "id": "df398b70dac2bd68",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T22:48:56.599470Z",
     "start_time": "2025-02-23T22:46:06.629429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# fp.load_state_dict(torch.load('models/fp.pth', map_location=torch.device('cpu'), weights_only=True), strict=False)\n",
    "fp = nfp()\n",
    "fp.eval()\n",
    "\n",
    "\n",
    "h = deque(maxlen=5000)\n",
    "\n",
    "for _ in tqdm(range(20)):\n",
    "\n",
    "\n",
    "    while len(h) < 500:\n",
    "        h += run_episode(fp)\n",
    "\n",
    "    nf = train_nn(h, fp, epochs = 3, lr=0.001, batch_size=64)\n",
    "\n",
    "    w, d, l = eval_nn(nf, fp)\n",
    "    print(w, d, l)\n",
    "    if w > 0.55:\n",
    "        print('Upgrade')\n",
    "        fp = nf\n"
   ],
   "id": "afa6891a727655df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07aa99442cd3463b8cf5c5946d8a5d03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m     h \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m run_episode(fp)\n\u001B[1;32m     14\u001B[0m nf \u001B[38;5;241m=\u001B[39m train_nn(h, fp, epochs \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m3\u001B[39m, lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m64\u001B[39m)\n\u001B[0;32m---> 16\u001B[0m w, d, l \u001B[38;5;241m=\u001B[39m \u001B[43meval_nn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28mprint\u001B[39m(w, d, l)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0.55\u001B[39m:\n",
      "Cell \u001B[0;32mIn[5], line 133\u001B[0m, in \u001B[0;36meval_nn\u001B[0;34m(fp, nf, n)\u001B[0m\n\u001B[1;32m    131\u001B[0m l \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n):\n\u001B[0;32m--> 133\u001B[0m     r \u001B[38;5;241m=\u001B[39m \u001B[43mrun_battle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m r \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    135\u001B[0m         w \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[5], line 93\u001B[0m, in \u001B[0;36mrun_battle\u001B[0;34m(fp, nf, mem_length, frames)\u001B[0m\n\u001B[1;32m     89\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(frames):\n\u001B[1;32m     92\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 93\u001B[0m         o, v \u001B[38;5;241m=\u001B[39m \u001B[43mfp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mto_emb\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m         a1 \u001B[38;5;241m=\u001B[39m o\u001B[38;5;241m.\u001B[39mexp()\u001B[38;5;241m.\u001B[39margmax()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     95\u001B[0m         y1 \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat([y1[:, \u001B[38;5;241m1\u001B[39m:, :], emb_mem([(b, a1)], ActionController\u001B[38;5;241m.\u001B[39mget_action_space())], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[3], line 54\u001B[0m, in \u001B[0;36mFP.forward\u001B[0;34m(self, x, h)\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, h):\n\u001B[0;32m---> 54\u001B[0m     nx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfl\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     nh \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfh(h) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpe[:h\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], :]\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mto(h\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m     56\u001B[0m     hidden \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdec(nx\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m), nh)\u001B[38;5;241m.\u001B[39mmean(dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/todd/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1743\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1739\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1741\u001B[0m \u001B[38;5;66;03m# torchrec tests the code consistency with the following code\u001B[39;00m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# fmt: off\u001B[39;00m\n\u001B[0;32m-> 1743\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_call_impl\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m   1744\u001B[0m     forward_call \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_slow_forward \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_get_tracing_state() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward)\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1746\u001B[0m     \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f70046284146470e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
