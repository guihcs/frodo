{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-06T22:39:49.652456Z",
     "start_time": "2025-02-06T22:39:47.330531Z"
    }
   },
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "from board2 import Board2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from controller import ActionController\n",
    "from tqdm.auto import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T22:39:49.677133Z",
     "start_time": "2025-02-06T22:39:49.664433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Node:\n",
    "    def __init__(self, board, player):\n",
    "        self.board = board\n",
    "        self.controller = ActionController(board)\n",
    "        self.children = []\n",
    "        self.p = player\n",
    "        self.qsa = 0\n",
    "        self.psa = 0\n",
    "        self.nsa = 0\n",
    "        self.a = None\n",
    "\n",
    "    def search(self, player):\n",
    "        winner = self.get_winner()\n",
    "        if winner is not None:\n",
    "            self.nsa += 1\n",
    "            return winner\n",
    "\n",
    "        if len(self.children) == 0:\n",
    "\n",
    "            actions = []\n",
    "            for a in self.controller.get_available_moves():\n",
    "                board = self.board.copy()\n",
    "                p = -self.p\n",
    "                n = Node(board, p)\n",
    "                n.a = a\n",
    "                n.controller.execute_action(a)\n",
    "                board.step()\n",
    "                actions.append(a)\n",
    "                self.children.append(n)\n",
    "\n",
    "            boards = torch.cat([x.board.grid.clone().unsqueeze(0) for x in self.children], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pl, vl = player(boards)\n",
    "\n",
    "            for i, (n, a) in enumerate(zip(self.children, actions)):\n",
    "\n",
    "                if n.get_winner() is not None:\n",
    "                    n.qsa = n.get_winner()\n",
    "                else:\n",
    "                    n.qsa = vl[i].item()\n",
    "                n.psa = pl.exp()[i, a].item()\n",
    "                n.board.swap_enemy()\n",
    "\n",
    "            self.qsa = torch.mean(vl).item()\n",
    "            self.nsa = len(self.children)\n",
    "\n",
    "            return self.qsa\n",
    "\n",
    "        best = max(self.children, key=lambda x: x.qsa * x.prob + 1.5 * x.psa * math.sqrt(sum([x.nsa for x in self.children]) / (x.nsa + 1)))\n",
    "\n",
    "        res = best.search(player)\n",
    "\n",
    "        self.qsa = (self.qsa * self.nsa + res) / (self.nsa + 1)\n",
    "        self.nsa += 1\n",
    "\n",
    "        return res\n",
    "\n",
    "    def simulate(self, player):\n",
    "        winner = self.get_winner()\n",
    "        if winner is not None:\n",
    "            return winner\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, vl = player(self.board.grid.clone().unsqueeze(0))\n",
    "        return vl.item()\n",
    "\n",
    "\n",
    "    def get_winner(self):\n",
    "\n",
    "        if self.controller.is_win():\n",
    "            return 1\n",
    "        elif self.controller.is_lose():\n",
    "            return -1\n",
    "        elif self.controller.is_block():\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    def get_improved_policy(self):\n",
    "        fp = [0 for x in range(self.controller.get_action_space())]\n",
    "        s = sum([x.nsa for x in self.children])\n",
    "        for x in self.children:\n",
    "            fp[x.action] = x.nsa / s\n",
    "        return fp\n",
    "\n",
    "    def get_policy(self):\n",
    "        s = sum([x.nsa for x in self.children])\n",
    "        return [x.nsa / s for x in self.children]"
   ],
   "id": "1d804ad14bd48218",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-06T22:40:28.459129Z",
     "start_time": "2025-02-06T22:40:28.447764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pos_encode(max_len, d_model, dtype=torch.float32):\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, d_model, dtype=dtype)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.pos = pos_encode(9, 16)\n",
    "\n",
    "        self.q = nn.Linear(16, 16)\n",
    "        self.k = nn.Linear(16, 16)\n",
    "        self.v = nn.Linear(16, 16)\n",
    "        self.n_heads = 16\n",
    "        self.n_dim = 16\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lc = nn.Linear(16, 16)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        wq = self._head_reshape(self.q(x))\n",
    "        wk = self._head_reshape(self.k(x))\n",
    "        wv = self._head_reshape(self.v(x))\n",
    "\n",
    "        a = wq @ wk.transpose(2, 3) / math.sqrt(self.n_dim)\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        o = a @ wv\n",
    "\n",
    "        return self.lc(self._head_reshape_back(o))\n",
    "\n",
    "    def _head_reshape(self, x):\n",
    "        return x.view(x.shape[0], x.shape[1], self.n_heads, self.n_dim // self.n_heads).transpose(1, 2)\n",
    "\n",
    "    def _head_reshape_back(self, x):\n",
    "        return x.transpose(1, 2).contiguous().view(x.shape[0], x.shape[2], self.n_dim)\n",
    "\n",
    "class Enc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enc, self).__init__()\n",
    "        self.attn = Attention()\n",
    "        self.ln1 = nn.LayerNorm(16)\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16)\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.attn(x)\n",
    "        h1 = self.ln1(h + x)\n",
    "        h2 = self.seq(h1)\n",
    "        return self.ln2(h1 + h2)\n",
    "\n",
    "class FrodoPolicy(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(FrodoPolicy, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.pos = pos_encode(9, 16)\n",
    "        #\n",
    "        # self.enc = nn.ModuleList([Enc() for _ in range(4)])\n",
    "        #\n",
    "        # self.pl = nn.Sequential(\n",
    "        #     nn.Linear(16, action_space),\n",
    "        #     nn.LogSoftmax(dim=-1)\n",
    "        # )\n",
    "        #\n",
    "        # self.vl = nn.Linear(16, 1)\n",
    "\n",
    "        self.enc = nn.Linear(144, 32)\n",
    "        self.pl = nn.Sequential(\n",
    "            nn.Linear(32, action_space),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.vl = nn.Linear(32, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = self.flatten(x)\n",
    "        h = self.enc(fx)\n",
    "        # px = self.pos.unsqueeze(0) + self.flatten(x)\n",
    "        #\n",
    "        # for enc in self.enc:\n",
    "        #     px = enc(px)\n",
    "        #\n",
    "        # cv = px.mean(dim=1)\n",
    "        #\n",
    "        # return self.pl(cv), self.vl(cv)\n",
    "        return self.pl(h), self.vl(h)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "board = Board2()\n",
    "controller = ActionController(board)\n",
    "#\n",
    "player = FrodoPolicy(controller.get_action_space())\n",
    "player.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = player(board.grid.clone().unsqueeze(0))"
   ],
   "id": "4dea4dc051af399",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T00:53:06.545293Z",
     "start_time": "2025-02-02T00:53:06.534435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_rewards(samples, p, winner):\n",
    "    if p == 1:\n",
    "        if winner == 1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(samples))]\n",
    "        elif winner == -1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(samples))]\n",
    "        else:\n",
    "            rewards = [0 for i in range(len(samples))]\n",
    "    else:\n",
    "        if winner == 1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(samples))]\n",
    "        elif winner == -1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(samples))]\n",
    "        else:\n",
    "            rewards = [0 for i in range(len(samples))]\n",
    "    return rewards\n",
    "\n",
    "def episode(player, moves = 100, mcts_iter = 50):\n",
    "    board = Board2()\n",
    "    samples = []\n",
    "    root = Node(board, -1)\n",
    "\n",
    "    for i in range(moves):\n",
    "        for _ in range(mcts_iter):\n",
    "            root.search(player)\n",
    "\n",
    "        samples.append((root.board, root.get_improved_policy()))\n",
    "        c = Categorical(torch.Tensor(root.get_improved_policy()))\n",
    "        move = c.sample().item()\n",
    "        for c in root.children:\n",
    "            if c.action == move:\n",
    "                root = c\n",
    "                break\n",
    "        else:\n",
    "            raise Exception('Invalid move')\n",
    "\n",
    "        if root.get_winner() is not None:\n",
    "            return samples, root.prob, root.get_winner()\n",
    "\n",
    "    return samples, root.p, 0\n",
    "\n",
    "def train_nn(player, dataset):\n",
    "    player.train()\n",
    "    optimizer = optim.Adam(player.parameters(), lr=0.001)\n",
    "    crit1 = nn.MSELoss()\n",
    "    crit2 = nn.CrossEntropyLoss()\n",
    "\n",
    "    lh = []\n",
    "    for e in range(10):\n",
    "        el = []\n",
    "        for b, p, r in DataLoader(dataset, batch_size=4, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            pl, vl = player(b)\n",
    "            loss = crit1(vl.squeeze(), r) + crit2(pl, p)\n",
    "            loss.backward()\n",
    "            el.append(loss.item())\n",
    "            optimizer.step()\n",
    "        lh.append(sum(el) / len(el))\n",
    "\n",
    "    return lh\n",
    "\n",
    "\n",
    "def battle(player, enemy):\n",
    "\n",
    "    player.eval()\n",
    "    enemy.eval()\n",
    "\n",
    "    board = Board2()\n",
    "    controller = ActionController(board)\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, _ = player(board.grid.clone().unsqueeze(0))\n",
    "        player_move = pl.exp().argmax().item()\n",
    "        controller.execute_action(player_move)\n",
    "        board.step()\n",
    "\n",
    "        if controller.is_win():\n",
    "            return 1\n",
    "        elif controller.is_block():\n",
    "            return 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, _ = enemy(board.grid.clone().unsqueeze(0))\n",
    "        enemy_move = pl.exp().argmax().item()\n",
    "        controller.execute_action(enemy_move)\n",
    "        board.step()\n",
    "\n",
    "        if controller.is_lose():\n",
    "            return -1\n",
    "        elif controller.is_block():\n",
    "            return 0\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_battle(player, enemy, n = 10):\n",
    "    wins, draws, loses = 0, 0, 0\n",
    "    for _ in range(n):\n",
    "        res = battle(player, enemy)\n",
    "        if res == 1:\n",
    "            wins += 1\n",
    "        elif res == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            loses += 1\n",
    "    return wins / n, draws / n, loses / n\n",
    "\n",
    "\n"
   ],
   "id": "803298856e4424d8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T01:15:46.993368Z",
     "start_time": "2025-02-02T00:53:15.553897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enemy = FrodoPolicy(ActionController(Board2()).get_action_space())\n",
    "player = FrodoPolicy(ActionController(Board2()).get_action_space())\n",
    "\n",
    "for e in tqdm(range(10)):\n",
    "    print('Iteration:', e)\n",
    "    data_boards = []\n",
    "    data_policies = []\n",
    "    data_rewards = []\n",
    "    player.eval()\n",
    "    enemy.eval()\n",
    "    for _ in range(5):\n",
    "\n",
    "        samples, p, winner = episode(player)\n",
    "\n",
    "        data_boards.extend([x[0].grid.clone().unsqueeze(0) for x in samples])\n",
    "        data_policies.extend([x[1] for x in samples])\n",
    "        data_rewards.extend(apply_rewards(samples, p, winner))\n",
    "\n",
    "    boards = torch.cat(data_boards, dim=0)\n",
    "    policies = torch.Tensor(data_policies)\n",
    "    rewards = torch.Tensor(data_rewards)\n",
    "\n",
    "\n",
    "    dataset = TensorDataset(boards, policies, rewards)\n",
    "\n",
    "    lh = train_nn(player, dataset)\n",
    "    print('Loss:', lh[-1])\n",
    "\n",
    "    wins, draws, loses = test_battle(player, enemy, 10)\n",
    "    print('Wins:', wins, 'Draws:', draws, 'Loses:', loses)\n",
    "    if wins > 0.55:\n",
    "        print('Enemy updated.')\n",
    "        enemy.load_state_dict(player.state_dict())\n",
    "\n"
   ],
   "id": "4907d1d7778930ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "602189d4d83247bdbcbd90ca24ba9be2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilherme/PycharmProjects/todd/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "/home/guilherme/PycharmProjects/todd/venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.238573760402446\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n",
      "Iteration: 1\n",
      "Loss: 3.242127052942912\n",
      "Wins: 0.1 Draws: 0.9 Loses: 0.0\n",
      "Iteration: 2\n",
      "Loss: 3.172249781001698\n",
      "Wins: 0.1 Draws: 0.9 Loses: 0.0\n",
      "Iteration: 3\n",
      "Loss: 3.089030446232976\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n",
      "Iteration: 4\n",
      "Loss: 3.0533790473015077\n",
      "Wins: 0.1 Draws: 0.9 Loses: 0.0\n",
      "Iteration: 5\n",
      "Loss: 2.971366214752197\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n",
      "Iteration: 6\n",
      "Loss: 3.09035378236037\n",
      "Wins: 0.1 Draws: 0.9 Loses: 0.0\n",
      "Iteration: 7\n",
      "Loss: 2.930754208564758\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n",
      "Iteration: 8\n",
      "Loss: 2.976498377950568\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n",
      "Iteration: 9\n",
      "Loss: 3.093575565253987\n",
      "Wins: 0.0 Draws: 1.0 Loses: 0.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-02T01:27:19.397467Z",
     "start_time": "2025-02-02T01:25:00.555466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "start = time.time()\n",
    "\n",
    "with torch.multiprocessing.Pool(4) as pool:\n",
    "    res = pool.map(episode, [player for _ in range(5)])\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Time:', end - start)"
   ],
   "id": "11aad2eee2a25ad2",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mmultiprocessing\u001B[38;5;241m.\u001B[39mPool(\u001B[38;5;241m4\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m pool:\n\u001B[0;32m----> 4\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mpool\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepisode\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m_\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m end \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mTime:\u001B[39m\u001B[38;5;124m'\u001B[39m, end \u001B[38;5;241m-\u001B[39m start)\n",
      "File \u001B[0;32m/usr/lib/python3.12/multiprocessing/pool.py:367\u001B[0m, in \u001B[0;36mPool.map\u001B[0;34m(self, func, iterable, chunksize)\u001B[0m\n\u001B[1;32m    362\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmap\u001B[39m(\u001B[38;5;28mself\u001B[39m, func, iterable, chunksize\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    363\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''\u001B[39;00m\n\u001B[1;32m    364\u001B[0m \u001B[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001B[39;00m\n\u001B[1;32m    365\u001B[0m \u001B[38;5;124;03m    in a list that is returned.\u001B[39;00m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;124;03m    '''\u001B[39;00m\n\u001B[0;32m--> 367\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_async\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapstar\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunksize\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.12/multiprocessing/pool.py:768\u001B[0m, in \u001B[0;36mApplyResult.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 768\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mready():\n\u001B[1;32m    770\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTimeoutError\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.12/multiprocessing/pool.py:765\u001B[0m, in \u001B[0;36mApplyResult.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    764\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwait\u001B[39m(\u001B[38;5;28mself\u001B[39m, timeout\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m--> 765\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_event\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.12/threading.py:655\u001B[0m, in \u001B[0;36mEvent.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    653\u001B[0m signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_flag\n\u001B[1;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m signaled:\n\u001B[0;32m--> 655\u001B[0m     signaled \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_cond\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    656\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m signaled\n",
      "File \u001B[0;32m/usr/lib/python3.12/threading.py:355\u001B[0m, in \u001B[0;36mCondition.wait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:    \u001B[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[39;00m\n\u001B[1;32m    354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 355\u001B[0m         \u001B[43mwaiter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43macquire\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    356\u001B[0m         gotit \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    357\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9de079583c420f4c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
