{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-01T20:08:49.820211Z",
     "start_time": "2025-02-01T20:08:47.280609Z"
    }
   },
   "source": [
    "from heapq import heappush, heappop\n",
    "\n",
    "from board2 import Board2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from controller import ActionController\n",
    "from tqdm.auto import tqdm\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T23:40:50.566616Z",
     "start_time": "2025-02-01T23:40:50.557751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Node:\n",
    "    def __init__(self, board, player):\n",
    "        self.board = board\n",
    "        self.controller = ActionController(board)\n",
    "        self.children = []\n",
    "        self.p = player\n",
    "        self.qsa = 0\n",
    "        self.psa = 0\n",
    "        self.nsa = 0\n",
    "        self.a = None\n",
    "\n",
    "    def search(self, player):\n",
    "        winner = self.get_winner()\n",
    "        if winner is not None:\n",
    "            self.nsa += 1\n",
    "            return winner\n",
    "\n",
    "        if len(self.children) == 0:\n",
    "\n",
    "            actions = []\n",
    "            for a in self.controller.get_available_moves():\n",
    "                board = self.board.copy()\n",
    "                p = -self.p\n",
    "                n = Node(board, p)\n",
    "                n.a = a\n",
    "                n.controller.execute_action(a)\n",
    "                board.step()\n",
    "                actions.append(a)\n",
    "                self.children.append(n)\n",
    "\n",
    "            boards = torch.cat([x.board.grid.clone().unsqueeze(0) for x in self.children], dim=0)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pl, vl = player(boards)\n",
    "\n",
    "            for i, (n, a) in enumerate(zip(self.children, actions)):\n",
    "\n",
    "                if n.get_winner() is not None:\n",
    "                    n.qsa = n.get_winner()\n",
    "                else:\n",
    "                    n.qsa = vl[i].item()\n",
    "                n.psa = pl.exp()[i, a].item()\n",
    "                n.board.swap_enemy()\n",
    "\n",
    "            self.qsa = torch.mean(vl).item()\n",
    "            self.nsa = len(self.children)\n",
    "\n",
    "            return self.qsa\n",
    "\n",
    "        best = max(self.children, key=lambda x: x.qsa * x.p + 1.5 * x.psa * math.sqrt(sum([x.nsa for x in self.children]) / (x.nsa + 1)))\n",
    "\n",
    "        res = best.search(player)\n",
    "\n",
    "        self.qsa = (self.qsa * self.nsa + res) / (self.nsa + 1)\n",
    "        self.nsa += 1\n",
    "\n",
    "        return res\n",
    "\n",
    "    def simulate(self, player):\n",
    "        winner = self.get_winner()\n",
    "        if winner is not None:\n",
    "            return winner\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, vl = player(self.board.grid.clone().unsqueeze(0))\n",
    "        return vl.item()\n",
    "\n",
    "\n",
    "    def get_winner(self):\n",
    "\n",
    "        if self.controller.is_win():\n",
    "            return 1\n",
    "        elif self.controller.is_lose():\n",
    "            return -1\n",
    "        elif self.controller.is_block():\n",
    "            return 0\n",
    "        return None\n",
    "\n",
    "    def get_improved_policy(self):\n",
    "        fp = [0 for x in range(self.controller.get_action_space())]\n",
    "        s = sum([x.nsa for x in self.children])\n",
    "        for x in self.children:\n",
    "            fp[x.a] = x.nsa / s\n",
    "        return fp\n",
    "\n",
    "    def get_policy(self):\n",
    "        s = sum([x.nsa for x in self.children])\n",
    "        return [x.nsa / s for x in self.children]"
   ],
   "id": "1d804ad14bd48218",
   "outputs": [],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T23:40:51.195379Z",
     "start_time": "2025-02-01T23:40:51.184412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def pos_encode(max_len, d_model, dtype=torch.float32):\n",
    "    position = torch.arange(max_len).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "    pe = torch.zeros(max_len, d_model, dtype=dtype)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.pos = pos_encode(9, 16)\n",
    "\n",
    "        self.q = nn.Linear(16, 16)\n",
    "        self.k = nn.Linear(16, 16)\n",
    "        self.v = nn.Linear(16, 16)\n",
    "        self.n_heads = 16\n",
    "        self.n_dim = 16\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.lc = nn.Linear(16, 16)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        wq = self._head_reshape(self.q(x))\n",
    "        wk = self._head_reshape(self.k(x))\n",
    "        wv = self._head_reshape(self.v(x))\n",
    "\n",
    "        a = wq @ wk.transpose(2, 3) / math.sqrt(self.n_dim)\n",
    "        a = torch.softmax(a, dim=-1)\n",
    "        a = self.dropout(a)\n",
    "\n",
    "        o = a @ wv\n",
    "\n",
    "        return self.lc(self._head_reshape_back(o))\n",
    "\n",
    "    def _head_reshape(self, x):\n",
    "        return x.view(x.shape[0], x.shape[1], self.n_heads, self.n_dim // self.n_heads).transpose(1, 2)\n",
    "\n",
    "    def _head_reshape_back(self, x):\n",
    "        return x.transpose(1, 2).contiguous().view(x.shape[0], x.shape[2], self.n_dim)\n",
    "\n",
    "class Enc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Enc, self).__init__()\n",
    "        self.attn = Attention()\n",
    "        self.ln1 = nn.LayerNorm(16)\n",
    "\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Linear(16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 16)\n",
    "        )\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.attn(x)\n",
    "        h1 = self.ln1(h + x)\n",
    "        h2 = self.seq(h1)\n",
    "        return self.ln2(h1 + h2)\n",
    "\n",
    "class FrodoPolicy(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(FrodoPolicy, self).__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        # self.pos = pos_encode(9, 16)\n",
    "        #\n",
    "        # self.enc = nn.ModuleList([Enc() for _ in range(4)])\n",
    "        #\n",
    "        # self.pl = nn.Sequential(\n",
    "        #     nn.Linear(16, action_space),\n",
    "        #     nn.LogSoftmax(dim=-1)\n",
    "        # )\n",
    "        #\n",
    "        # self.vl = nn.Linear(16, 1)\n",
    "\n",
    "        self.enc = nn.Linear(144, 32)\n",
    "        self.pl = nn.Sequential(\n",
    "            nn.Linear(32, action_space),\n",
    "            nn.LogSoftmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.vl = nn.Linear(32, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        fx = self.flatten(x)\n",
    "        h = self.enc(fx)\n",
    "        # px = self.pos.unsqueeze(0) + self.flatten(x)\n",
    "        #\n",
    "        # for enc in self.enc:\n",
    "        #     px = enc(px)\n",
    "        #\n",
    "        # cv = px.mean(dim=1)\n",
    "        #\n",
    "        # return self.pl(cv), self.vl(cv)\n",
    "        return self.pl(h), self.vl(h)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# board = Board2()\n",
    "# controller = ActionController(board)\n",
    "#\n",
    "# player = FrodoPolicy(controller.get_action_space())\n",
    "# print(player(board.grid.clone().unsqueeze(0)).exp().shape)"
   ],
   "id": "4dea4dc051af399",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T23:40:51.871667Z",
     "start_time": "2025-02-01T23:40:51.859572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def apply_rewards(samples, p, winner):\n",
    "    if p == 1:\n",
    "        if winner == 1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(samples))]\n",
    "        elif winner == -1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(samples))]\n",
    "        else:\n",
    "            rewards = [0 for i in range(len(samples))]\n",
    "    else:\n",
    "        if winner == 1:\n",
    "            rewards = [-1 if i % 2 == 0 else 1 for i in range(len(samples))]\n",
    "        elif winner == -1:\n",
    "            rewards = [1 if i % 2 == 0 else -1 for i in range(len(samples))]\n",
    "        else:\n",
    "            rewards = [0 for i in range(len(samples))]\n",
    "    return rewards\n",
    "\n",
    "def episode(player):\n",
    "    board = Board2()\n",
    "    samples = []\n",
    "    root = Node(board, -1)\n",
    "\n",
    "    for i in range(100):\n",
    "        for _ in range(50):\n",
    "            root.search(player)\n",
    "\n",
    "        samples.append((root.board, root.get_improved_policy()))\n",
    "        c = Categorical(torch.Tensor(root.get_improved_policy()))\n",
    "        move = c.sample().item()\n",
    "        for c in root.children:\n",
    "            if c.a == move:\n",
    "                root = c\n",
    "                break\n",
    "        else:\n",
    "            raise Exception('Invalid move')\n",
    "\n",
    "        if root.get_winner() is not None:\n",
    "            return samples, root.p, root.get_winner()\n",
    "\n",
    "    return samples, root.p, 0\n",
    "\n",
    "def train_nn(player, dataset):\n",
    "    player.train()\n",
    "    optimizer = optim.Adam(player.parameters(), lr=0.001)\n",
    "    crit1 = nn.MSELoss()\n",
    "    crit2 = nn.CrossEntropyLoss()\n",
    "\n",
    "    lh = []\n",
    "    for e in range(10):\n",
    "        el = []\n",
    "        for b, p, r in DataLoader(dataset, batch_size=4, shuffle=True):\n",
    "            optimizer.zero_grad()\n",
    "            pl, vl = player(b)\n",
    "            loss = crit1(vl.squeeze(), r) + crit2(pl, p)\n",
    "            loss.backward()\n",
    "            el.append(loss.item())\n",
    "            optimizer.step()\n",
    "        lh.append(sum(el) / len(el))\n",
    "\n",
    "    return lh\n",
    "\n",
    "\n",
    "def battle(player, enemy):\n",
    "\n",
    "    player.eval()\n",
    "    enemy.eval()\n",
    "\n",
    "    board = Board2()\n",
    "    controller = ActionController(board)\n",
    "\n",
    "    for _ in range(100):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, _ = player(board.grid.clone().unsqueeze(0))\n",
    "        player_move = pl.exp().argmax().item()\n",
    "        controller.execute_action(player_move)\n",
    "        board.step()\n",
    "\n",
    "        if controller.is_win():\n",
    "            return 1\n",
    "        elif controller.is_block():\n",
    "            return 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pl, _ = enemy(board.grid.clone().unsqueeze(0))\n",
    "        enemy_move = pl.exp().argmax().item()\n",
    "        controller.execute_action(enemy_move)\n",
    "        board.step()\n",
    "\n",
    "        if controller.is_lose():\n",
    "            return -1\n",
    "        elif controller.is_block():\n",
    "            return 0\n",
    "\n",
    "    return 0\n",
    "\n",
    "\n",
    "def test_battle(player, enemy, n = 10):\n",
    "    wins, draws, loses = 0, 0, 0\n",
    "    for _ in range(n):\n",
    "        res = battle(player, enemy)\n",
    "        if res == 1:\n",
    "            wins += 1\n",
    "        elif res == 0:\n",
    "            draws += 1\n",
    "        else:\n",
    "            loses += 1\n",
    "    return wins / n, draws / n, loses / n\n",
    "\n",
    "\n"
   ],
   "id": "803298856e4424d8",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-01T23:42:26.339121Z",
     "start_time": "2025-02-01T23:41:14.648297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "enemy = FrodoPolicy(ActionController(Board2()).get_action_space())\n",
    "player = FrodoPolicy(ActionController(Board2()).get_action_space())\n",
    "\n",
    "for _ in range(2):\n",
    "\n",
    "    data_boards = []\n",
    "    data_policies = []\n",
    "    data_rewards = []\n",
    "    player.eval()\n",
    "    enemy.eval()\n",
    "    for _ in range(1):\n",
    "\n",
    "        samples, p, winner = episode(player)\n",
    "\n",
    "        data_boards.extend([x[0].grid.clone().unsqueeze(0) for x in samples])\n",
    "        data_policies.extend([x[1] for x in samples])\n",
    "        data_rewards.extend(apply_rewards(samples, p, winner))\n",
    "\n",
    "    boards = torch.cat(data_boards, dim=0)\n",
    "    policies = torch.Tensor(data_policies)\n",
    "    rewards = torch.Tensor(data_rewards)\n",
    "\n",
    "\n",
    "    dataset = TensorDataset(boards, policies, rewards)\n",
    "\n",
    "    train_nn(player, dataset)\n",
    "\n",
    "    wins, draws, loses = test_battle(player, enemy, 10)\n",
    "    print(wins, draws, loses)\n",
    "    if wins > 0.55:\n",
    "        enemy.load_state_dict(player.state_dict())\n",
    "\n"
   ],
   "id": "4907d1d7778930ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0\n",
      "0.0 1.0 0.0\n"
     ]
    }
   ],
   "execution_count": 170
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "11aad2eee2a25ad2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
