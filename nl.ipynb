{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T23:42:34.910652Z",
     "start_time": "2025-01-28T23:42:34.905490Z"
    }
   },
   "source": [
    "from board2 import Board2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "from controller import ActionController\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-28T23:56:13.166513Z",
     "start_time": "2025-01-28T23:56:13.152719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FrodoPolicy(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(FrodoPolicy, self).__init__()\n",
    "\n",
    "        self.nl = nn.Sequential(\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(144, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, action_space),\n",
    "            nn.LogSoftmax(dim=-1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nl(x)\n",
    "\n",
    "\n",
    "class GameRunner:\n",
    "\n",
    "    def run(self, player, enemy, runs=650, explore=True, explore_rate=0.1):\n",
    "        value = 0\n",
    "        history = []\n",
    "        board = Board2()\n",
    "        controller = ActionController(board)\n",
    "        for i in range(runs):\n",
    "\n",
    "            if controller.is_win():\n",
    "                value = 1\n",
    "                break\n",
    "            elif controller.is_lose():\n",
    "                value = -1\n",
    "                break\n",
    "            elif controller.is_block():\n",
    "                value = -0.5\n",
    "                break\n",
    "\n",
    "            with torch.no_grad():\n",
    "                state = board.grid.clone().unsqueeze(0)\n",
    "\n",
    "                if explore and random.random() < explore_rate:\n",
    "                    player_action = random.randint(0, controller.get_action_space() - 1)\n",
    "                else:\n",
    "                    player_action = torch.argmax(player(state).exp()).item()\n",
    "                history.append((state, player_action))\n",
    "                controller.execute_action(player_action)\n",
    "                enemy_action = enemy(state).exp()\n",
    "                board.swap_enemy()\n",
    "                controller.execute_action(torch.argmax(enemy_action).item())\n",
    "                board.swap_enemy()\n",
    "                board.step()\n",
    "\n",
    "        return history, value\n",
    "\n",
    "    def test(self, player, enemy, runs=650, battles=10):\n",
    "        wins = 0\n",
    "        losses = 0\n",
    "        draws = 0\n",
    "\n",
    "        for _ in range(battles):\n",
    "            history, value = self.run(player, enemy, runs, explore=False)\n",
    "            if value == 1:\n",
    "                wins += 1\n",
    "            elif value == -1:\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "\n",
    "        return wins, losses, draws\n",
    "\n",
    "class FrodoTrainer:\n",
    "\n",
    "    def train(self, runs=100):\n",
    "        board = Board2()\n",
    "        controller = ActionController(board)\n",
    "\n",
    "        player = FrodoPolicy(controller.get_action_space())\n",
    "        enemy = FrodoPolicy(controller.get_action_space())\n",
    "\n",
    "        game_runner = GameRunner()\n",
    "\n",
    "        for i in tqdm(range(runs)):\n",
    "            self.train_iteration(player, enemy, game_runner, controller)\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                wins, losses, draws = game_runner.test(player, enemy)\n",
    "                print(f'Wins: {wins}, Losses: {losses}, Draws: {draws}')\n",
    "\n",
    "    def train_iteration(self, player, enemy, game_runner, controller):\n",
    "        train_boards = []\n",
    "        train_actions = []\n",
    "\n",
    "        for _ in range(10):\n",
    "            history, value = game_runner.run(player, enemy)\n",
    "\n",
    "            boards, actions = zip(*history)\n",
    "\n",
    "            if value < 1:\n",
    "                actions = random.choices(list(range(0, controller.get_action_space())), k=len(actions))\n",
    "\n",
    "            train_boards.extend(boards)\n",
    "            train_actions.extend(actions)\n",
    "\n",
    "\n",
    "        dataset = TensorDataset(torch.cat(train_boards, dim=0), torch.LongTensor(train_actions))\n",
    "\n",
    "        crit = nn.NLLLoss()\n",
    "        optimizer = optim.Adam(player.parameters(), lr=0.001)\n",
    "        lh = []\n",
    "        for e in range(100):\n",
    "            el = 0\n",
    "            for b, a in DataLoader(dataset, batch_size=32):\n",
    "                player.zero_grad()\n",
    "                output = player(b)\n",
    "                loss = crit(output, a)\n",
    "                loss.backward()\n",
    "                el += loss.item()\n",
    "                optimizer.step()\n",
    "            lh.append(el / len(dataset))\n",
    "\n",
    "trainer = FrodoTrainer()"
   ],
   "id": "46fbe3b089758686",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-29T00:01:28.959585Z",
     "start_time": "2025-01-28T23:56:14.819234Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train(500)",
   "id": "b4126a242964a79",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9ec7aaba9368443d92569d1cf4fa4206"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wins: 0, Losses: 1, Draws: 9\n",
      "Wins: 0, Losses: 0, Draws: 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[49], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m500\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[48], line 93\u001B[0m, in \u001B[0;36mFrodoTrainer.train\u001B[0;34m(self, runs)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_iteration(player, enemy, game_runner, controller)\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 93\u001B[0m     wins, losses, draws \u001B[38;5;241m=\u001B[39m \u001B[43mgame_runner\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menemy\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mWins: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mwins\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Losses: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlosses\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Draws: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdraws\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n",
      "Cell \u001B[0;32mIn[48], line 68\u001B[0m, in \u001B[0;36mGameRunner.test\u001B[0;34m(self, player, enemy, runs, battles)\u001B[0m\n\u001B[1;32m     65\u001B[0m draws \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(battles):\n\u001B[0;32m---> 68\u001B[0m     history, value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mplayer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menemy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mruns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexplore\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     70\u001B[0m         wins \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "Cell \u001B[0;32mIn[48], line 58\u001B[0m, in \u001B[0;36mGameRunner.run\u001B[0;34m(self, player, enemy, runs, explore, explore_rate)\u001B[0m\n\u001B[1;32m     56\u001B[0m         controller\u001B[38;5;241m.\u001B[39mexecute_action(torch\u001B[38;5;241m.\u001B[39margmax(enemy_action)\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     57\u001B[0m         board\u001B[38;5;241m.\u001B[39mswap_enemy()\n\u001B[0;32m---> 58\u001B[0m         \u001B[43mboard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m history, value\n",
      "File \u001B[0;32m~/PycharmProjects/todd/board2.py:419\u001B[0m, in \u001B[0;36mBoard2.step\u001B[0;34m(self, time_step, walk_time)\u001B[0m\n\u001B[1;32m    416\u001B[0m py, px \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_enemy_position()\n\u001B[1;32m    418\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_adjacent(ey, ex, py, px):\n\u001B[0;32m--> 419\u001B[0m     paths \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshortest_path\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mey\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitem\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    420\u001B[0m     remaining_time \u001B[38;5;241m=\u001B[39m time_step\n\u001B[1;32m    421\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m paths \u001B[38;5;129;01mand\u001B[39;00m remaining_time \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/todd/board2.py:265\u001B[0m, in \u001B[0;36mBoard2.shortest_path\u001B[0;34m(self, y1, x1, y2, x2, max_steps)\u001B[0m\n\u001B[1;32m    262\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sqr_distance(y, end[\u001B[38;5;241m0\u001B[39m], x, end[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[1;32m    263\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m p[\u001B[38;5;241m1\u001B[39m:]\n\u001B[0;32m--> 265\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ny, nx \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_empty_neighbours\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    266\u001B[0m     nc \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sqr_distance(y, ny, x, nx) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m6\u001B[39m\n\u001B[1;32m    267\u001B[0m     cost \u001B[38;5;241m=\u001B[39m oc \u001B[38;5;241m+\u001B[39m nc \u001B[38;5;241m+\u001B[39m sqr_distance(ny, end[\u001B[38;5;241m0\u001B[39m], nx, end[\u001B[38;5;241m1\u001B[39m])\n",
      "File \u001B[0;32m~/PycharmProjects/todd/board2.py:137\u001B[0m, in \u001B[0;36mBoard2.get_empty_neighbours\u001B[0;34m(self, y, x)\u001B[0m\n\u001B[1;32m    135\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m    136\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m j \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y \u001B[38;5;241m+\u001B[39m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y \u001B[38;5;241m+\u001B[39m i \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m3\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m x \u001B[38;5;241m+\u001B[39m j \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m x \u001B[38;5;241m+\u001B[39m j \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m3\u001B[39m: \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_cell_empty\u001B[49m\u001B[43m(\u001B[49m\u001B[43my\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mj\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    138\u001B[0m             neighbours\u001B[38;5;241m.\u001B[39mappend((y \u001B[38;5;241m+\u001B[39m i, x \u001B[38;5;241m+\u001B[39m j))\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m neighbours\n",
      "File \u001B[0;32m~/PycharmProjects/todd/board2.py:129\u001B[0m, in \u001B[0;36mBoard2.is_cell_empty\u001B[0;34m(self, y, x)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_cell_empty\u001B[39m(\u001B[38;5;28mself\u001B[39m, y, x):\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrid[FLOOR_LAYER, y, x]\u001B[38;5;241m.\u001B[39many() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgrid\u001B[49m\u001B[43m[\u001B[49m\u001B[43mTODD_LAYER\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43many\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrid[PLAYER_LAYER, y, x]\u001B[38;5;241m.\u001B[39many() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \\\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrid[ENEMY_LAYER, y, x]\u001B[38;5;241m.\u001B[39many() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgrid[\u001B[38;5;241m0\u001B[39m, y, x]\u001B[38;5;241m.\u001B[39many()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d61b614c259750ea"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
